# Reinforcement Learning for Agentic AI Systems

**Multi-Agent Content Creation with Deep Q-Networks and Thompson Sampling**

[![Python](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red.svg)](https://pytorch.org/)
[![License](https://img.shields.io/badge/license-MIT-green.svg)](LICENSE)

**Author:** Saurabh Soni  
**Course:** INFO 7375 - Prompt Engineering for Generative AI  
**Institution:** Northeastern University  
**Date:** December 10, 2025

---

## ðŸŽ¯ Project Overview

This project implements a reinforcement learning-enhanced multi-agent orchestration system that learns optimal coordination strategies for specialized AI agents. Using **Deep Q-Networks (DQN)** and **Thompson Sampling**, the system achieves **16.18% improvement** over baseline performance while maintaining **94.2% quality**.

### Key Features

- âœ… **Two RL Approaches**: DQN (value-based learning) + Thompson Sampling (exploration strategy)
- âœ… **5 Coordination Patterns**: Sequential, Parallel, Hierarchical, Collaborative, Adaptive
- âœ… **4 Specialized Agents**: Research, Writing, Editor, Technical
- âœ… **Novel Exploration**: Three-phase strategy ensuring pattern diversity
- âœ… **Multi-Objective Rewards**: Balances quality, efficiency, coordination, diversity
- âœ… **Statistical Validation**: Proven improvement (p < 0.001)

---

## ðŸ“Š Results Summary (500 Episodes)

| Metric | Value |
|--------|-------|
| **Average Reward** | 0.9286 (92.9%) |
| **Final Performance** | 0.9649 (96.5%) |
| **Average Quality** | 0.9420 (94.2%) |
| **Improvement** | 16.18% |
| **Pattern Diversity** | 96% (all 5 patterns used) |
| **Statistical Significance** | p < 0.001 |
| **Best Episode** | #256 (perfect 1.0 reward) |

---

## ðŸš€ Quick Start

### Prerequisites

- Python 3.8 or higher
- pip package manager

### Installation

```bash
# 1. Clone or download the project
cd rl-agentic-ai-system

# 2. Create virtual environment (recommended)
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# 3. Install dependencies
pip install -r requirements.txt
```

### Run Training

```bash
# Train for 200 episodes (recommended)
python main.py
# When prompted, enter: 200

# Or use default 200 episodes
python main.py
# Just press Enter
```

### Run Tests

```bash
# Verify installation
python test_system.py

# Should see: âœ“ ALL TESTS PASSED!
```

### Generate Sample Content

```bash
# See what coordinated agents would produce
python content_generator.py
# Check: output/content/ folder
```

---

## ðŸ“ Project Structure

```
rl-agentic-ai-system/
â”œâ”€â”€ main.py                     # Main entry point
â”œâ”€â”€ rl_orchestrator.py          # RL-based agent coordination
â”œâ”€â”€ dqn_agent.py               # Deep Q-Network implementation
â”œâ”€â”€ thompson_sampler.py        # Thompson Sampling & UCB
â”œâ”€â”€ agents.py                  # 4 specialized agents
â”œâ”€â”€ reward_function.py         # Multi-objective reward calculation
â”œâ”€â”€ config.py                  # Configuration management
â”œâ”€â”€ visualization.py           # Results visualization
â”œâ”€â”€ evaluation.py              # Performance evaluation
â”œâ”€â”€ test_system.py             # Test suite
â”œâ”€â”€ content_generator.py       # Sample content generation
â”œâ”€â”€ preserve_outputs.py        # Backup utility
â”œâ”€â”€ requirements.txt           # Dependencies
â”œâ”€â”€ README.md                  # This file
â”‚
â””â”€â”€ output/                    # Generated during training
    â”œâ”€â”€ plots/                 # Visualizations (PNG)
    â”œâ”€â”€ results/               # Metrics (JSON)
    â”œâ”€â”€ models/                # Saved RL models
    â”œâ”€â”€ content/               # Sample generated content
    â””â”€â”€ logs/                  # Training logs
```

---

## ðŸ§  System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         RL Agent System                 â”‚
â”‚                                         â”‚
â”‚  DQN Agent â†â†’ State Encoder             â”‚
â”‚      â†“              â†“                   â”‚
â”‚  RL Orchestrator                        â”‚
â”‚  â€¢ Pattern Selection (5 types)          â”‚
â”‚  â€¢ Three-Phase Exploration              â”‚
â”‚  â€¢ Multi-Objective Rewards              â”‚
â”‚      â†“              â†“                   â”‚
â”‚  Thompson Sampler â†â†’ Agent Team         â”‚
â”‚  â€¢ Beta(Î±,Î²) per agent                  â”‚
â”‚  â€¢ Bayesian Selection                   â”‚
â”‚                                         â”‚
â”‚  4 Specialized Agents:                  â”‚
â”‚  â€¢ Research  â€¢ Writing                  â”‚
â”‚  â€¢ Editor    â€¢ Technical                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ðŸ”¬ Technical Approach

### Approach 1: Deep Q-Networks (DQN)

**Purpose:** Learn which coordination pattern to use for each task

**Components:**
- Q-Network: [256â†’128â†’64] hidden layers
- Experience Replay: 10,000 capacity
- Target Network: Updated every 10 steps
- Epsilon-Greedy: Îµ = 1.0 â†’ 0.01 (decay: 0.97)

**State Space:** 32 dimensions (task features, agent states, context)  
**Action Space:** 5 coordination patterns

### Approach 2: Thompson Sampling

**Purpose:** Select best agents within coordination patterns

**Mechanism:**
- Beta distribution per agent: Î¸áµ¢ ~ Beta(Î±áµ¢, Î²áµ¢)
- Sample and select: i* = argmax(Î¸Ì‚áµ¢)
- Bayesian update: Î± += 1 (success), Î² += 1 (failure)

**Integration:** Used in Hierarchical and Adaptive patterns

---

## ðŸ“ˆ Key Innovation: Three-Phase Exploration

| Phase | Episodes | Strategy | Purpose |
|-------|----------|----------|---------|
| **Forced** | 1-30 | Cycle all patterns | Guarantee exploration |
| **Guided** | 31-60 | 50% DQN, 50% random | Safe transition |
| **Normal** | 61+ | Epsilon-greedy DQN | Exploit learned policy |

**Impact:** Increased pattern diversity from 0% â†’ 96%

---

## ðŸŽ“ Usage Examples

### Basic Training

```python
from rl_orchestrator import RLAgentOrchestrator
from config import Config

# Initialize
config = Config()
orchestrator = RLAgentOrchestrator(config)

# Execute single task
task = {
    'type': 'blog_post',
    'topic': 'AI Trends',
    'requirements': {
        'length': 800,
        'tone': 'informative',
        'target_audience': 'general'
    }
}

result = orchestrator.execute_task(task)
print(f"Reward: {result['reward']:.3f}")
print(f"Quality: {result['quality_score']:.3f}")
print(f"Pattern Used: {result['coordination_pattern']}")
```

### Backup Your Results

```bash
# Before running new experiments, backup current results
python preserve_outputs.py backup 500ep

# List all backups
python preserve_outputs.py list

# Restore if needed
python preserve_outputs.py restore output_backup_500ep
```

---

## ðŸ“Š Understanding the Outputs

### Training Outputs

After training, check `output/` folder:

**1. Visualizations** (`output/plots/`)
- `training_curves.png` - Learning progress over episodes
- `agent_utilization.png` - Which agents were used (balanced 21-28%)
- `exploration_rate.png` - Epsilon decay (1.0 â†’ 0.01)
- `final_results.png` - Complete 6-panel summary

**2. Metrics** (`output/results/`)
- `final_evaluation.json` - All 28 performance metrics
  - Average reward, quality, improvement percentage
  - Statistical test results (t-test, p-value)
  - Convergence analysis

**3. Models** (`output/models/`)
- `model_ep500_dqn.pth` - Trained DQN neural network
- Can load and deploy for production

**4. Sample Content** (`output/content/`)
- Examples of what coordinated agents would produce
- Shows different patterns in action

---

## ðŸ”§ Configuration

Edit `config.py` or create `config.json`:

```python
{
    "num_episodes": 200,           # Training episodes
    "dqn_learning_rate": 0.001,    # DQN learning rate
    "dqn_epsilon_decay": 0.97,     # Epsilon decay rate
    "reward_quality_weight": 0.4,  # Quality importance
    "reward_diversity_weight": 0.2 # Diversity importance
}
```

---

## ðŸ“ Assignment Requirements

**This project fulfills:**

âœ… **Two RL Approaches**: DQN + Thompson Sampling  
âœ… **Agentic System Integration**: Multi-agent orchestration  
âœ… **Complete Implementation**: 2,700+ lines of production code  
âœ… **Comprehensive Testing**: Full test suite included  
âœ… **Experimental Design**: 500 episodes with statistical validation  
âœ… **Results Analysis**: Learning curves, pattern analysis, significance tests  
âœ… **Documentation**: Technical report, setup guides, code comments  

---

## ðŸ› Troubleshooting

**Issue:** `ModuleNotFoundError: No module named 'torch'`
```bash
pip install torch numpy scipy matplotlib seaborn
```

**Issue:** Tests failing
```bash
rm -rf __pycache__
python test_system.py
```

**Issue:** Want to run quick test
```bash
python main.py
# Enter: 20  (just 20 episodes for testing)
```

---

## ðŸ“š Documentation

- **Technical Report**: See `FINAL_TECHNICAL_REPORT.md` (or PDF version)
- **Quick Start**: See `QUICKSTART.md` for 5-minute setup
- **Experimental Design**: See `EXPERIMENTAL_DESIGN.md` for methodology
- **Demo Script**: See `PRACTICAL_DEMO_SCRIPT.md` for presentation guide

---

## ðŸŽ¯ Key Results

### Pattern Performance

| Pattern | Usage | Avg Reward | Best For |
|---------|-------|------------|----------|
| **Adaptive** | 21.2% | **0.945** | Complex/uncertain tasks |
| **Hierarchical** | 14.0% | **0.938** | Coordination-intensive |
| **Collaborative** | 21.4% | 0.925 | Iterative refinement |
| **Sequential** | 23.4% | 0.921 | Simple linear tasks |
| **Parallel** | 20.0% | 0.914 | Independent subtasks |

### Agent Performance

- Agent 0 (Research): 21.7% usage, 0.928 quality
- Agent 1 (Writing): 23.7% usage, 0.945 quality
- **Agent 2 (Editor): 27.0% usage, 0.958 quality** â† Best
- Agent 3 (Technical): 27.6% usage, 0.941 quality

---

## ðŸ”® Future Enhancements

- **Real LLM Integration**: Replace simulated agents with GPT-4/Claude
- **Continuous Actions**: Fine-grained coordination control
- **Multi-Task Learning**: Transfer across domains
- **Meta-Learning**: Fast adaptation to new tasks
- **Hierarchical RL**: Temporal abstraction

---

## ðŸ“„ Citation

If you use this work, please cite:

```bibtex
@project{soni2025rl,
  author = {Saurabh Soni},
  title = {Reinforcement Learning for Agentic AI Systems},
  year = {2025},
  institution = {Northeastern University},
  course = {INFO 7375 - Prompt Engineering for Generative AI}
}
```

---

## ðŸ“§ Contact

**Saurabh Soni**  
Northeastern University  
Course: INFO 7375 - Prompt Engineering for Generative AI

---

## ðŸ“œ License

This project is created for academic purposes as part of INFO 7375 coursework.
